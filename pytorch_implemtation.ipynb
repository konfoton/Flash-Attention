{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "150f609a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== B=128 H=16 ==\n",
      "L=1024 | math failed: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 23.56 GiB of which 221.38 MiB is free. Including non-PyTorch memory, this process has 23.32 GiB memory in use. Of the allocated memory 23.01 GiB is allocated by PyTorch, and 11.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "L=1024 | flash=16.651417541503907 ms | math=    None ms\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
    "\n",
    "D = 128\n",
    "warmup = 3\n",
    "reps = 5\n",
    "seqs = [1024]\n",
    "bh_sets = [(128,16)]\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "dtype = torch.float16 if use_cuda else torch.float32\n",
    "\n",
    "torch.manual_seed(0)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "\n",
    "def bench_sdpa(B, H, L, backend, warmup, reps):\n",
    "    q = torch.randn(B, H, L, D, device=device, dtype=dtype)\n",
    "    k = torch.randn(B, H, L, D, device=device, dtype=dtype)\n",
    "    v = torch.randn(B, H, L, D, device=device, dtype=dtype)\n",
    "\n",
    "    if not use_cuda:\n",
    "        import time\n",
    "        t0 = time.perf_counter()\n",
    "        for _ in range(warmup):\n",
    "            _ = F.scaled_dot_product_attention(q, k, v, dropout_p=0.0, is_causal=False)\n",
    "        t1 = time.perf_counter()\n",
    "        for _ in range(reps):\n",
    "            _ = F.scaled_dot_product_attention(q, k, v, dropout_p=0.0, is_causal=False)\n",
    "        t2 = time.perf_counter()\n",
    "        warmup_ms = (t1 - t0) * 1e3\n",
    "        avg_ms = (t2 - t1) * 1e3 / reps\n",
    "        return avg_ms, None\n",
    "    else:\n",
    "        if backend == 'flash':\n",
    "            backends = [SDPBackend.FLASH_ATTENTION]\n",
    "        elif backend == 'mem_efficient':\n",
    "            backends = [SDPBackend.EFFICIENT_ATTENTION]\n",
    "        elif backend == 'math':\n",
    "            backends = [SDPBackend.MATH]\n",
    "        else:\n",
    "            raise ValueError('Unknown backend')\n",
    "\n",
    "        with sdpa_kernel(backends):\n",
    "            for _ in range(warmup):\n",
    "                _ = F.scaled_dot_product_attention(q, k, v, dropout_p=0.0, is_causal=False)\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "            start = torch.cuda.Event(enable_timing=True)\n",
    "            end = torch.cuda.Event(enable_timing=True)\n",
    "            sum_time = 0\n",
    "            for _ in range(reps):\n",
    "                start.record()\n",
    "                _ = F.scaled_dot_product_attention(q, k, v, dropout_p=0.0, is_causal=False)\n",
    "                end.record()\n",
    "                torch.cuda.synchronize()\n",
    "                sum_time += start.elapsed_time(end)\n",
    "        return sum_time / reps, (q, k, v)\n",
    "\n",
    "for (B, H) in bh_sets:\n",
    "    print(f\"\\n== B={B} H={H} ==\")\n",
    "    for L in seqs:\n",
    "        try:\n",
    "            f_ms, tensors = bench_sdpa(B, H, L, 'flash', warmup, reps)\n",
    "        except Exception as e:\n",
    "            print(f\"L={L:4d} | flash failed: {e}\")\n",
    "            f_ms, tensors = None, None\n",
    "\n",
    "        try:\n",
    "            m_ms, tensors_m = bench_sdpa(B, H, L, 'math', warmup, reps)\n",
    "        except Exception as e:\n",
    "            print(f\"L={L:4d} | math failed: {e}\")\n",
    "            m_ms, tensors_m = None, None\n",
    "\n",
    "        if use_cuda and tensors is not None and tensors_m is not None:\n",
    "            q, k, v = tensors\n",
    "            q2, k2, v2 = tensors_m\n",
    "            with sdpa_kernel([SDPBackend.FLASH_ATTENTION]):\n",
    "                out_f = F.scaled_dot_product_attention(q, k, v, dropout_p=0.0, is_causal=False)\n",
    "            with sdpa_kernel([SDPBackend.MATH]):\n",
    "                out_m = F.scaled_dot_product_attention(q2, k2, v2, dropout_p=0.0, is_causal=False)\n",
    "\n",
    "        print(f\"L={L:4d} | flash={f_ms!s:>8} ms | math={m_ms!s:>8} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec8000d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a588ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
