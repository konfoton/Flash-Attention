{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "150f609a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== B=2 H=4 ==\n",
      "L= 128 | flash=0.027820798754692077 ms | math=0.16079360246658325 ms\n",
      "L= 256 | flash=0.026848000288009644 ms | math=0.16028800010681152 ms\n",
      "L= 512 | flash=0.03256320059299469 ms | math=0.2091007947921753 ms\n",
      "L=1024 | flash=0.05509120225906372 ms | math=0.5545983791351319 ms\n",
      "\n",
      "== B=4 H=8 ==\n",
      "L= 128 | flash=0.02682879865169525 ms | math=0.15912959575653077 ms\n",
      "L= 256 | flash=0.02622080147266388 ms | math=0.19537919759750366 ms\n",
      "L= 512 | flash=0.05631999969482422 ms | math=0.5799680233001709 ms\n",
      "L=1024 | flash=0.18350080251693726 ms | math=1.9929088592529296 ms\n",
      "\n",
      "== B=8 H=8 ==\n",
      "L= 128 | flash=0.0268095999956131 ms | math=0.159334397315979 ms\n",
      "L= 256 | flash=0.03583999872207642 ms | math=0.3483648061752319 ms\n",
      "L= 512 | flash=0.1000704050064087 ms | math=1.0936320304870606 ms\n",
      "L=1024 | flash=0.3090431928634644 ms | math=3.7894142150878904 ms\n",
      "\n",
      "== B=8 H=16 ==\n",
      "L= 128 | flash=0.027238398790359497 ms | math=0.22525439262390137 ms\n",
      "L= 256 | flash=0.058982402086257935 ms | math=0.6461184024810791 ms\n",
      "L= 512 | flash=0.16521600484848023 ms | math=2.0553728103637696 ms\n",
      "L=1024 | flash=0.5611839771270752 ms | math=7.406950378417969 ms\n",
      "\n",
      "== B=16 H=16 ==\n",
      "L= 128 | flash=0.04382719993591309 ms | math=0.43024640083312987 ms\n",
      "L= 256 | flash=0.09966080188751221 ms | math=1.2122112274169923 ms\n",
      "L= 512 | flash=0.2987839937210083 ms | math=3.986841583251953 ms\n",
      "L=1024 | flash=1.0852352142333985 ms | math=14.728192138671876 ms\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
    "\n",
    "D = 64\n",
    "warmup = 3\n",
    "reps = 5\n",
    "seqs = [128, 256, 512, 1024]\n",
    "bh_sets = [(2,4), (4,8), (8,8), (8,16), (16,16)]\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "dtype = torch.float16 if use_cuda else torch.float32\n",
    "\n",
    "torch.manual_seed(0)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "\n",
    "def bench_sdpa(B, H, L, backend, warmup, reps):\n",
    "    q = torch.randn(B, H, L, D, device=device, dtype=dtype)\n",
    "    k = torch.randn(B, H, L, D, device=device, dtype=dtype)\n",
    "    v = torch.randn(B, H, L, D, device=device, dtype=dtype)\n",
    "\n",
    "    if not use_cuda:\n",
    "        import time\n",
    "        t0 = time.perf_counter()\n",
    "        for _ in range(warmup):\n",
    "            _ = F.scaled_dot_product_attention(q, k, v, dropout_p=0.0, is_causal=False)\n",
    "        t1 = time.perf_counter()\n",
    "        for _ in range(reps):\n",
    "            _ = F.scaled_dot_product_attention(q, k, v, dropout_p=0.0, is_causal=False)\n",
    "        t2 = time.perf_counter()\n",
    "        warmup_ms = (t1 - t0) * 1e3\n",
    "        avg_ms = (t2 - t1) * 1e3 / reps\n",
    "        return avg_ms, None\n",
    "    else:\n",
    "        if backend == 'flash':\n",
    "            backends = [SDPBackend.FLASH_ATTENTION]\n",
    "        elif backend == 'mem_efficient':\n",
    "            backends = [SDPBackend.EFFICIENT_ATTENTION]\n",
    "        elif backend == 'math':\n",
    "            backends = [SDPBackend.MATH]\n",
    "        else:\n",
    "            raise ValueError('Unknown backend')\n",
    "\n",
    "        with sdpa_kernel(backends):\n",
    "            for _ in range(warmup):\n",
    "                _ = F.scaled_dot_product_attention(q, k, v, dropout_p=0.0, is_causal=False)\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "            start = torch.cuda.Event(enable_timing=True)\n",
    "            end = torch.cuda.Event(enable_timing=True)\n",
    "            start.record()\n",
    "            for _ in range(reps):\n",
    "                _ = F.scaled_dot_product_attention(q, k, v, dropout_p=0.0, is_causal=False)\n",
    "            end.record()\n",
    "            torch.cuda.synchronize()\n",
    "            avg_ms = start.elapsed_time(end) / reps\n",
    "        return avg_ms, (q, k, v)\n",
    "\n",
    "for (B, H) in bh_sets:\n",
    "    print(f\"\\n== B={B} H={H} ==\")\n",
    "    for L in seqs:\n",
    "        try:\n",
    "            f_ms, tensors = bench_sdpa(B, H, L, 'flash', warmup, reps)\n",
    "        except Exception as e:\n",
    "            print(f\"L={L:4d} | flash failed: {e}\")\n",
    "            f_ms, tensors = None, None\n",
    "\n",
    "        try:\n",
    "            m_ms, tensors_m = bench_sdpa(B, H, L, 'math', warmup, reps)\n",
    "        except Exception as e:\n",
    "            print(f\"L={L:4d} | math failed: {e}\")\n",
    "            m_ms, tensors_m = None, None\n",
    "\n",
    "        if use_cuda and tensors is not None and tensors_m is not None:\n",
    "            q, k, v = tensors\n",
    "            q2, k2, v2 = tensors_m\n",
    "            with sdpa_kernel([SDPBackend.FLASH_ATTENTION]):\n",
    "                out_f = F.scaled_dot_product_attention(q, k, v, dropout_p=0.0, is_causal=False)\n",
    "            with sdpa_kernel([SDPBackend.MATH]):\n",
    "                out_m = F.scaled_dot_product_attention(q2, k2, v2, dropout_p=0.0, is_causal=False)\n",
    "\n",
    "        print(f\"L={L:4d} | flash={f_ms!s:>8} ms | math={m_ms!s:>8} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec8000d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
