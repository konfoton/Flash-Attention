{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b281d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/konrad/Flash-Attention/flash_attention_tensor_cores/libflash_attention.so\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "/home/konrad/anaconda3/lib/python3.13/site-packages/zmq/backend/cython/../../../../.././libstdc++.so.6: version `GLIBCXX_3.4.32' not found (required by /home/konrad/Flash-Attention/flash_attention_tensor_cores/libflash_attention.so)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m lib_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mgetcwd(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibflash_attention.so\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(lib_path)\n\u001b[0;32m----> 8\u001b[0m lib \u001b[38;5;241m=\u001b[39m ct\u001b[38;5;241m.\u001b[39mCDLL(lib_path)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Define types\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# __half* → use POINTER(c_uint16) (binary-compatible with float16)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m HalfPtr \u001b[38;5;241m=\u001b[39m ct\u001b[38;5;241m.\u001b[39mPOINTER(ct\u001b[38;5;241m.\u001b[39mc_uint16)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/ctypes/__init__.py:390\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m _dlopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, mode)\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: /home/konrad/anaconda3/lib/python3.13/site-packages/zmq/backend/cython/../../../../.././libstdc++.so.6: version `GLIBCXX_3.4.32' not found (required by /home/konrad/Flash-Attention/flash_attention_tensor_cores/libflash_attention.so)"
     ]
    }
   ],
   "source": [
    "import ctypes as ct\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load the shared library (adjust path if needed)\n",
    "lib_path = os.path.join(os.getcwd(), \"libflash_attention.so\")\n",
    "lib = ct.CDLL(lib_path)\n",
    "\n",
    "# Define types\n",
    "# __half* → use POINTER(c_uint16) (binary-compatible with float16)\n",
    "HalfPtr = ct.POINTER(ct.c_uint16)\n",
    "FloatPtr = ct.POINTER(ct.c_float)\n",
    "\n",
    "# Function signatures (extern \"C\" in wrapper.cuh)\n",
    "# void run_tensor_flash_attention_host_half(\n",
    "#     const __half* hQ, const __half* hK, const __half* hV, __half* hO,\n",
    "#     int B, int H, int L, int D, int tile, cudaStream_t stream = 0, float* elapsed_ms = nullptr);\n",
    "lib.run_tensor_flash_attention_host_half.argtypes = [\n",
    "    HalfPtr, HalfPtr, HalfPtr, HalfPtr,\n",
    "    ct.c_int, ct.c_int, ct.c_int, ct.c_int, ct.c_int,\n",
    "    ct.c_void_p,  # stream (nullptr)\n",
    "    ct.POINTER(ct.c_float)  # elapsed_ms (nullable)\n",
    "]\n",
    "lib.run_tensor_flash_attention_host_half.restype = None\n",
    "\n",
    "# Prepare inputs\n",
    "B, H, L, D, tile = 1, 16, 64, 128, 64\n",
    "size = B * H * L * D\n",
    "\n",
    "# Create numpy float16 buffers\n",
    "Q = (np.random.randn(size).astype(np.float16))\n",
    "K = (np.random.randn(size).astype(np.float16))\n",
    "V = (np.random.randn(size).astype(np.float16))\n",
    "O = np.zeros(size, dtype=np.float16)\n",
    "\n",
    "# Get ctypes pointers (uint16 underlying storage)\n",
    "Q_p = Q.ctypes.data_as(HalfPtr)\n",
    "K_p = K.ctypes.data_as(HalfPtr)\n",
    "V_p = V.ctypes.data_as(HalfPtr)\n",
    "O_p = O.ctypes.data_as(HalfPtr)\n",
    "\n",
    "# elapsed time capture (optional)\n",
    "elapsed = ct.c_float(0.0)\n",
    "elapsed_p = ct.pointer(elapsed)\n",
    "\n",
    "# Call the function (stream = None/0)\n",
    "lib.run_tensor_flash_attention_host_half(\n",
    "    Q_p, K_p, V_p, O_p,\n",
    "    B, H, L, D, tile,\n",
    "    None,\n",
    "    elapsed_p\n",
    ")\n",
    "\n",
    "print(\"Elapsed ms:\", elapsed.value)\n",
    "print(\"Output sample:\", O[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c325f741",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
