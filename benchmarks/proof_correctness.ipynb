{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2589b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed ms: 7.799744129180908\n",
      "Output sample: [-0.0359   -0.06085  -0.002062 -0.0999   -0.1879   -0.015526  0.1666\n",
      "  0.06696   0.03867  -0.1951  ]\n"
     ]
    }
   ],
   "source": [
    "import ctypes as ct\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load the shared library (adjust path if needed)\n",
    "lib_path = os.path.join(os.getcwd(), \"libflash_attention.so\")\n",
    "lib = ct.CDLL(lib_path)\n",
    "\n",
    "# Define types\n",
    "# __half* â†’ use POINTER(c_uint16) (binary-compatible with float16)\n",
    "HalfPtr = ct.POINTER(ct.c_uint16)\n",
    "FloatPtr = ct.POINTER(ct.c_float)\n",
    "\n",
    "# Function signatures (extern \"C\" in wrapper.cuh)\n",
    "# void run_tensor_flash_attention_host_half(\n",
    "#     const __half* hQ, const __half* hK, const __half* hV, __half* hO,\n",
    "#     int B, int H, int L, int D, int tile, cudaStream_t stream = 0, float* elapsed_ms = nullptr);\n",
    "lib.run_tensor_flash_attention_host_half.argtypes = [\n",
    "    HalfPtr, HalfPtr, HalfPtr, HalfPtr,\n",
    "    ct.c_int, ct.c_int, ct.c_int, ct.c_int, ct.c_int,\n",
    "    ct.c_void_p,  # stream (nullptr)\n",
    "    ct.POINTER(ct.c_float)  # elapsed_ms (nullable)\n",
    "]\n",
    "lib.run_tensor_flash_attention_host_half.restype = None\n",
    "\n",
    "# Prepare inputs\n",
    "B, H, L, D, tile = 32, 16, 512, 128, 64\n",
    "size = B * H * L * D\n",
    "\n",
    "# Create numpy float16 buffers\n",
    "Q = (np.random.randn(size).astype(np.float16))\n",
    "K = (np.random.randn(size).astype(np.float16))\n",
    "V = (np.random.randn(size).astype(np.float16))\n",
    "O = np.zeros(size, dtype=np.float16)\n",
    "\n",
    "# Get ctypes pointers (uint16 underlying storage)\n",
    "Q_p = Q.ctypes.data_as(HalfPtr)\n",
    "K_p = K.ctypes.data_as(HalfPtr)\n",
    "V_p = V.ctypes.data_as(HalfPtr)\n",
    "O_p = O.ctypes.data_as(HalfPtr)\n",
    "\n",
    "# elapsed time capture (optional)\n",
    "elapsed = ct.c_float(0.0)\n",
    "elapsed_p = ct.pointer(elapsed)\n",
    "\n",
    "# Call the function (stream = None/0)\n",
    "lib.run_tensor_flash_attention_host_half(\n",
    "    Q_p, K_p, V_p, O_p,\n",
    "    B, H, L, D, tile,\n",
    "    None,\n",
    "    elapsed_p\n",
    ")\n",
    "\n",
    "print(\"Elapsed ms:\", elapsed.value)\n",
    "print(\"Output sample:\", O[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff62864d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Ensure PyTorch uses GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "# Reshape the flat buffers into (B, H, L, D)\n",
    "Q_t = torch.from_numpy(Q.reshape(B, H, L, D).astype(np.float16)).to(device)\n",
    "K_t = torch.from_numpy(K.reshape(B, H, L, D).astype(np.float16)).to(device)\n",
    "V_t = torch.from_numpy(V.reshape(B, H, L, D).astype(np.float16)).to(device)\n",
    "\n",
    "# Compute scaled dot-product attention: softmax((Q @ K^T) / sqrt(D)) @ V\n",
    "scale = 1.0 / np.sqrt(D)\n",
    "# (B,H,L,D) x (B,H,D,L) -> (B,H,L,L)\n",
    "scores = torch.matmul(Q_t.to(torch.float32), K_t.transpose(-1, -2).to(torch.float32)) * scale\n",
    "attn = torch.softmax(scores, dim=-1)\n",
    "O_ref = torch.matmul(attn, V_t.to(torch.float32))  # (B,H,L,D) in float32\n",
    "O_ref = O_ref.to(torch.float16)\n",
    "\n",
    "# Flatten to match O's layout\n",
    "O_ref_np = O_ref.detach().cpu().numpy().reshape(-1)\n",
    "\n",
    "# Compare against kernel output O (numpy float16 flat)\n",
    "O_np = O  # already numpy float16 flat\n",
    "\n",
    "# Compute mean absolute error\n",
    "mae = np.mean(np.abs(O_ref_np.astype(np.float32) - O_np.astype(np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4bb0b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Ensure we have reference and output arrays\n",
    "assert 'O_ref_np' in globals() and 'O' in globals(), \"Reference or output not found\"\n",
    "O_kernel = O.astype(np.float32)\n",
    "O_ref32 = O_ref_np.astype(np.float32)\n",
    "\n",
    "# Elementwise differences\n",
    "diff = O_ref32 - O_kernel\n",
    "abs_diff = np.abs(diff)\n",
    "\n",
    "# Metrics\n",
    "mae = float(np.mean(abs_diff))\n",
    "# Mean relative error (avoid divide-by-zero: mask zeros in reference)\n",
    "nonzero_mask = np.abs(O_ref32) > 0\n",
    "if np.any(nonzero_mask):\n",
    "    mre = float(np.mean(np.abs(diff[nonzero_mask]) / np.abs(O_ref32[nonzero_mask])))\n",
    "else:\n",
    "    mre = float('nan')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14b354f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Ensure tensors on CUDA and in FP16\n",
    "assert 'Q_t' in globals() and 'K_t' in globals() and 'V_t' in globals(), \"Run the PyTorch prep cell first\"\n",
    "\n",
    "# Enable PyTorch Flash SDP when available\n",
    "try:\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True  # optional for perf\n",
    "    torch.backends.cuda.enable_flash_sdp(True)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Use scaled_dot_product_attention which routes to FlashAttention on supported GPUs\n",
    "# Input shapes: (B, H, L, D)\n",
    "Q16 = Q_t.to(torch.float16)\n",
    "K16 = K_t.to(torch.float16)\n",
    "V16 = V_t.to(torch.float16)\n",
    "\n",
    "# PyTorch API expects (B,H,L,D) and will compute attention along L\n",
    "O_flash16 = torch.nn.functional.scaled_dot_product_attention(\n",
    "    Q16, K16, V16, attn_mask=None, dropout_p=0.0, is_causal=False\n",
    ")\n",
    "\n",
    "# Flatten and move to CPU\n",
    "O_flash16_np = O_flash16.detach().cpu().numpy().reshape(-1).astype(np.float16)\n",
    "\n",
    "# Compare FP16 directly (kernel output O is float16)\n",
    "O_kernel16 = O.astype(np.float16)\n",
    "diff16 = O_flash16_np.astype(np.float16).astype(np.float32) - O_kernel16.astype(np.float16).astype(np.float32)\n",
    "abs_diff16 = np.abs(diff16)\n",
    "\n",
    "mae16 = float(np.mean(abs_diff16))\n",
    "# Mean relative error in FP16 domain (compute in float32 to avoid underflow)\n",
    "ref32 = O_flash16_np.astype(np.float32)\n",
    "nonzero_mask16 = np.abs(ref32) > 0\n",
    "mre16 = float(np.mean(np.abs(diff16[nonzero_mask16]) / np.abs(ref32[nonzero_mask16]))) if np.any(nonzero_mask16) else float('nan')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2100755c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# NumPy FlashAttention (blockwise, stable softmax) implementation\n",
    "# Contract:\n",
    "# - Inputs: Q, K, V shaped (B, H, L, D), dtype float16/float32\n",
    "# - tile: integer tile length along sequence L\n",
    "# - Returns: O shaped (B, H, L, D), dtype float32 (optionally cast to float16)\n",
    "# - Behavior: computes softmax(QK^T / sqrt(D)) @ V in a numerically stable, streaming manner\n",
    "\n",
    "def flash_attention_numpy(Q_bhl_d, K_bhl_d, V_bhl_d, tile: int, return_fp16: bool = True):\n",
    "    assert Q_bhl_d.ndim == 4 and K_bhl_d.ndim == 4 and V_bhl_d.ndim == 4, \"Expect (B,H,L,D)\"\n",
    "    B, H, L, D = Q_bhl_d.shape\n",
    "    assert K_bhl_d.shape == (B, H, L, D)\n",
    "    assert V_bhl_d.shape == (B, H, L, D)\n",
    "    assert tile > 0 and tile <= L\n",
    "\n",
    "    # Work in float32 for stability; we'll cast at the end if requested\n",
    "    Q = Q_bhl_d.astype(np.float32, copy=False)\n",
    "    K = K_bhl_d.astype(np.float32, copy=False)\n",
    "    V = V_bhl_d.astype(np.float32, copy=False)\n",
    "\n",
    "    scale = 1.0 / np.sqrt(float(D))\n",
    "\n",
    "    # Output accumulator (float32)\n",
    "    O = np.zeros((B, H, L, D), dtype=np.float32)\n",
    "\n",
    "    # We compute attention per query position i, streaming over key/value tiles.\n",
    "    # Maintain per-(b,h,i) running max m_i and normalization factor l_i, plus value accumulator o_i.\n",
    "    # Reference: FlashAttention algorithm (Dao et al.).\n",
    "\n",
    "    # Initialize m_i (max logits), l_i (sum of exp shifted), and o_i (accumulated values)\n",
    "    m_i = np.full((B, H, L), -np.inf, dtype=np.float32)\n",
    "    l_i = np.zeros((B, H, L), dtype=np.float32)\n",
    "    o_i = np.zeros((B, H, L, D), dtype=np.float32)\n",
    "\n",
    "    # Iterate over K/V tiles along sequence\n",
    "    for start in range(0, L, tile):\n",
    "        end = min(start + tile, L)\n",
    "        K_tile = K[:, :, start:end, :]   # (B,H,T,D)\n",
    "        V_tile = V[:, :, start:end, :]   # (B,H,T,D)\n",
    "\n",
    "        # Compute scores for this tile: (B,H,L,D) @ (B,H,D,T) -> (B,H,L,T)\n",
    "        # We'll batch-matmul via einsum to avoid huge temp memory\n",
    "        # s_ij = (Q_i dot K_j) * scale\n",
    "        S = np.einsum('bhld,bhTd->bh lT', Q, K_tile, optimize=True) * scale  # shapes: l=D axis name; T=end-start\n",
    "\n",
    "        # Current tile-wise max per query index i\n",
    "        m_ij = np.max(S, axis=-1)  # (B,H,L)\n",
    "\n",
    "        # Combine with running max\n",
    "        m_new = np.maximum(m_i, m_ij)  # (B,H,L)\n",
    "\n",
    "        # Compute exp of shifted scores wrt new max: exp(S - m_new[..., None])\n",
    "        S_shift = S - m_new[..., None]\n",
    "        P = np.exp(S_shift)  # (B,H,L,T)\n",
    "\n",
    "        # l_new = exp(m_i - m_new) * l_i + sum_j exp(S - m_new)\n",
    "        alpha = np.exp(m_i - m_new)  # (B,H,L)\n",
    "        l_new = alpha * l_i + np.sum(P, axis=-1)  # (B,H,L)\n",
    "\n",
    "        # o_new = (alpha * o_i) + sum_j P_ij * V_j\n",
    "        # sum over tile positions j: (B,H,L,T) with (B,H,T,D) -> (B,H,L,D)\n",
    "        PV = np.einsum('bh lT,bhTd->bh ld', P, V_tile, optimize=True)\n",
    "        o_new = alpha[..., None] * o_i + PV  # (B,H,L,D)\n",
    "\n",
    "        # Update running states\n",
    "        m_i = m_new\n",
    "        l_i = l_new\n",
    "        o_i = o_new\n",
    "\n",
    "    # Final output: O_i = o_i / l_i\n",
    "    O = o_i / l_i[..., None]\n",
    "\n",
    "    if return_fp16:\n",
    "        return O.astype(np.float16)\n",
    "    return O\n",
    "\n",
    "# If Q,K,V,O,B,H,L,D are available from previous cells, run comparisons\n",
    "try:\n",
    "    assert 'Q' in globals() and 'K' in globals() and 'V' in globals(), 'Run the first cell to define Q,K,V.'\n",
    "    B_, H_, L_, D_ = B, H, L, D\n",
    "    Q4 = Q.reshape(B_, H_, L_, D_)\n",
    "    K4 = K.reshape(B_, H_, L_, D_)\n",
    "    V4 = V.reshape(B_, H_, L_, D_)\n",
    "\n",
    "    # Choose a tile size (match kernel tile if known)\n",
    "    tile_size = 64\n",
    "\n",
    "    # NumPy FlashAttention output\n",
    "    O_np_flash16 = flash_attention_numpy(Q4, K4, V4, tile=tile_size, return_fp16=True)\n",
    "    O_np_flash32 = O_np_flash16.astype(np.float32)\n",
    "\n",
    "    # Kernel output and PyTorch reference prepared earlier\n",
    "    O_kernel16 = O.astype(np.float16)\n",
    "    O_kernel32 = O_kernel16.astype(np.float32)\n",
    "\n",
    "    # Compare NumPy FlashAttention vs kernel (FP16 domain measured in fp32)\n",
    "    diff_k = O_np_flash32.reshape(-1) - O_kernel32.reshape(-1)\n",
    "    abs_diff_k = np.abs(diff_k)\n",
    "    mae_k = float(np.mean(abs_diff_k))\n",
    "    ref32_k = O_kernel32.reshape(-1)\n",
    "    nz_k = np.abs(ref32_k) > 0\n",
    "    mre_k = float(np.mean(np.abs(diff_k[nz_k]) / np.abs(ref32_k[nz_k]))) if np.any(nz_k) else float('nan')\n",
    "\n",
    "\n",
    "    # Compare NumPy FlashAttention vs PyTorch Flash SDP if available\n",
    "    if 'O_flash16_np' in globals():\n",
    "        O_torch32 = O_flash16_np.astype(np.float32)\n",
    "        diff_t = O_np_flash32.reshape(-1) - O_torch32.reshape(-1)\n",
    "        abs_diff_t = np.abs(diff_t)\n",
    "        mae_t = float(np.mean(abs_diff_t))\n",
    "        ref32_t = O_torch32.reshape(-1)\n",
    "        nz_t = np.abs(ref32_t) > 0\n",
    "        mre_t = float(np.mean(np.abs(diff_t[nz_t]) / np.abs(ref32_t[nz_t]))) if np.any(nz_t) else float('nan')\n",
    "\n",
    "    else:\n",
    "        print('PyTorch Flash SDP output not found; run the PyTorch Flash cell first to compare.')\n",
    "except Exception as e:\n",
    "    print('Skip comparisons until inputs are prepared:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97a0c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel_vs_numpy\n",
      "  MAE: 6.86005296302028e-05\n",
      "  MRE: 0.008922836743295193\n",
      "kernel_vs_torch\n",
      "  MAE: 7.20318712410517e-05\n",
      "  MRE: 0.009019171819090843\n",
      "numpy_vs_torch\n",
      "  MAE: 1.023419918055879e-05\n",
      "  MRE: 0.0016797683201730251\n"
     ]
    }
   ],
   "source": [
    "# Summary: Precision comparison\n",
    "# - Kernel vs NumPy FlashAttention\n",
    "# - Kernel vs PyTorch Flash SDP\n",
    "# - NumPy vs PyTorch Flash SDP\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "summary = {}\n",
    "\n",
    "# Kernel vs NumPy\n",
    "if 'O_np_flash32' in globals():\n",
    "    O_kernel32 = O.astype(np.float32)\n",
    "    diff_kn = O_kernel32.reshape(-1) - O_np_flash32.reshape(-1)\n",
    "    ad_kn = np.abs(diff_kn)\n",
    "    summary['kernel_vs_numpy'] = {\n",
    "        'MAE': float(np.mean(ad_kn)),\n",
    "        'MRE': float(np.mean(np.abs(diff_kn[np.abs(O_kernel32.reshape(-1))>0]) / np.abs(O_kernel32.reshape(-1)[np.abs(O_kernel32.reshape(-1))>0]))) if np.any(np.abs(O_kernel32.reshape(-1))>0) else float('nan'),\n",
    "    }\n",
    "\n",
    "# Kernel vs PyTorch\n",
    "if 'O_flash16_np' in globals():\n",
    "    O_torch32 = O_flash16_np.astype(np.float32)\n",
    "    diff_kt = O_kernel32.reshape(-1) - O_torch32.reshape(-1)\n",
    "    ad_kt = np.abs(diff_kt)\n",
    "    summary['kernel_vs_torch'] = {\n",
    "        'MAE': float(np.mean(ad_kt)),\n",
    "        'MRE': float(np.mean(np.abs(diff_kt[np.abs(O_torch32.reshape(-1))>0]) / np.abs(O_torch32.reshape(-1)[np.abs(O_torch32.reshape(-1))>0]))) if np.any(np.abs(O_torch32.reshape(-1))>0) else float('nan'),\n",
    "    }\n",
    "\n",
    "# NumPy vs PyTorch\n",
    "if 'O_np_flash32' in globals() and 'O_flash16_np' in globals():\n",
    "    O_torch32 = O_flash16_np.astype(np.float32)\n",
    "    diff_nt = O_np_flash32.reshape(-1) - O_torch32.reshape(-1)\n",
    "    ad_nt = np.abs(diff_nt)\n",
    "    summary['numpy_vs_torch'] = {\n",
    "        'MAE': float(np.mean(ad_nt)),\n",
    "        'MRE': float(np.mean(np.abs(diff_nt[np.abs(O_torch32.reshape(-1))>0]) / np.abs(O_torch32.reshape(-1)[np.abs(O_torch32.reshape(-1))>0]))) if np.any(np.abs(O_torch32.reshape(-1))>0) else float('nan'),\n",
    "    }\n",
    "    \n",
    "for k, v in summary.items():\n",
    "    print(k)\n",
    "    for mk, mv in v.items():\n",
    "        print(f\"  {mk}: {mv}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
