{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "150f609a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B=32 H= 16 L= 512 | flash=1.677 ms | TFLOPS = 40.97500513046423\n",
      "B=16 H= 16 L=1024 | flash=2.712 ms | TFLOPS = 50.67760280600406\n",
      "B=8 H= 16 L=2048 | flash=4.529 ms | TFLOPS = 60.68763895319586\n",
      "B=4 H= 16 L=4096 | flash=8.527 ms | TFLOPS = 64.4738494384939\n",
      "B=2 H= 16 L=8192 | flash=16.84 ms | TFLOPS = 65.30799629394922\n",
      "B=1 H= 16 L=16384 | flash=31.06 ms | TFLOPS = 70.79427202352494\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
    "\n",
    "D = 128\n",
    "H = 16\n",
    "seqs = [512, 1024, 2048, 4096, 8192, 16384]\n",
    "b = [32, 16, 8, 4, 2, 1]\n",
    "warmup = 3\n",
    "reps = 5\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "dtype = torch.float16\n",
    "torch.manual_seed(0)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "\n",
    "def bench_sdpa(B, H, L, backend, warmup, reps):\n",
    "    q = torch.randn(B, H, L, D, device=device, dtype=dtype)\n",
    "    k = torch.randn(B, H, L, D, device=device, dtype=dtype)\n",
    "    v = torch.randn(B, H, L, D, device=device, dtype=dtype)\n",
    "\n",
    "    if not use_cuda:\n",
    "        import time\n",
    "        t0 = time.perf_counter()\n",
    "        for _ in range(warmup):\n",
    "            _ = F.scaled_dot_product_attention(q, k, v, dropout_p=0.0, is_causal=False)\n",
    "        t1 = time.perf_counter()\n",
    "        for _ in range(reps):\n",
    "            _ = F.scaled_dot_product_attention(q, k, v, dropout_p=0.0, is_causal=False)\n",
    "        t2 = time.perf_counter()\n",
    "        warmup_ms = (t1 - t0) * 1e3\n",
    "        avg_ms = (t2 - t1) * 1e3 / reps\n",
    "        return avg_ms, None\n",
    "    else:\n",
    "        if backend == 'flash':\n",
    "            backends = [SDPBackend.FLASH_ATTENTION]\n",
    "        else:\n",
    "            raise ValueError('Unknown backend')\n",
    "\n",
    "        with sdpa_kernel(backends):\n",
    "            for _ in range(warmup):\n",
    "                _ = F.scaled_dot_product_attention(q, k, v, dropout_p=0.0, is_causal=False)\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "            start = torch.cuda.Event(enable_timing=True)\n",
    "            end = torch.cuda.Event(enable_timing=True)\n",
    "            sum_time = 0\n",
    "            for _ in range(reps):\n",
    "                start.record()\n",
    "                _ = F.scaled_dot_product_attention(q, k, v, dropout_p=0.0, is_causal=False)\n",
    "                end.record()\n",
    "                torch.cuda.synchronize()\n",
    "                sum_time += start.elapsed_time(end)\n",
    "        return sum_time / reps, (q, k, v)\n",
    "\n",
    "for (B, L) in zip(b, seqs):\n",
    "    try:\n",
    "        f_ms, tensors = bench_sdpa(B, H, L, 'flash', warmup, reps)\n",
    "    except Exception as e:\n",
    "        print(f\"L={L:4d} | flash failed: {e}\")\n",
    "        f_ms, tensors = None, None\n",
    "\n",
    "    flops = 4.0 * L * L * D * H * B\n",
    "    tflops = (flops / (f_ms / 1000.0)) / 1e12\n",
    "    print(f\"B={B} H= {H} L={L:4d} | flash={f_ms:.4} ms | TFLOPS = {tflops}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
